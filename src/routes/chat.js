import { Hono } from 'hono';
import { OpenAIService } from '../services/openai.js';
import { VectorizeService } from '../services/vectorize.js';
import { parseSSEStream, createSSEResponse, createErrorResponse } from '../utils/responses.js';
import { validateChatMessages, validateOptions, sanitizeInput } from '../utils/validation.js';

const chat = new Hono();

chat.post('/', async (c) => {
  try {
    const { messages, options = {} } = await c.req.json();

    // Basic validation
    if (!messages || !Array.isArray(messages)) {
      return c.json(createErrorResponse('Messages array is required'), 400);
    }

    validateChatMessages(messages);
    validateOptions(options);

    // Sanitize messages
    const sanitizedMessages = messages.map(msg => ({
      ...msg,
      content: sanitizeInput(msg.content)
    }));

    // Check API key
    if (!c.env.OPENAI_API_KEY) {
      return c.json(createErrorResponse('OpenAI API key not configured'), 500);
    }

    // Initialize services
    const openai = new OpenAIService(c.env);
    const vectorize = new VectorizeService(c.env.CONTENT_VECTORIZE, openai);

    // Get the last user message for content search
    const lastUserMessage = [...sanitizedMessages].reverse().find(msg => msg.role === 'user');
    let enhancedMessages = [...sanitizedMessages];

    if (lastUserMessage && c.env.CONTENT_VECTORIZE) {
      try {
        // Search for relevant content
        const searchOptions = options.contentId ? { contentId: options.contentId } : {};
        const contextResult = await vectorize.getContentContext(lastUserMessage.content, 3, searchOptions);

        if (contextResult.hasContext) {
          // Add context to system message or create new one
          const contextPrompt = `ê´€ë ¨ ê°•ì˜ ìë£Œ:
${contextResult.context}

ìœ„ ê°•ì˜ ìë£Œë¥¼ ì°¸ê³ í•˜ì—¬ í•™ìŠµìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ê°•ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆëŠ” ê²½ìš° ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.

IMPORTANT: ë‹µë³€ ì‹œì‘ ë¶€ë¶„ì— ë°˜ë“œì‹œ "ğŸ“š ê°•ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤.\n\n"ë¥¼ í¬í•¨í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì´ê²ƒì´ ê°•ì˜ ê¸°ë°˜ ë‹µë³€ì„ì„ ì•Œë ¤ì£¼ì„¸ìš”.`;

          const hasSystemMessage = enhancedMessages.some(msg => msg.role === 'system');

          if (hasSystemMessage) {
            // Enhance existing system message
            const systemIndex = enhancedMessages.findIndex(msg => msg.role === 'system');
            enhancedMessages[systemIndex] = {
              ...enhancedMessages[systemIndex],
              content: enhancedMessages[systemIndex].content + '\n\n' + contextPrompt
            };
          } else {
            // Add new system message with context
            enhancedMessages.unshift({
              role: 'system',
              content: contextPrompt
            });
          }

          console.log(`âœ… Found ${contextResult.relevantChunks} relevant content chunks for query`);
        } else {
          // No relevant content found - return rejection message directly without AI processing
          console.log('â„¹ï¸ No relevant content found - returning rejection message directly');

          const rejectionMessage = 'ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë“±ë¡ëœ ê°•ì˜ ìë£Œì—ì„œëŠ” í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°•ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.';

          // Create a simple SSE stream with the rejection message
          const rejectionStream = new ReadableStream({
            start(controller) {
              const sseData = `data: ${JSON.stringify({
                choices: [{
                  delta: { content: rejectionMessage }
                }]
              })}\n\n`;
              controller.enqueue(new TextEncoder().encode(sseData));
              controller.enqueue(new TextEncoder().encode('data: [DONE]\n\n'));
              controller.close();
            }
          });

          const parsedRejectionStream = parseSSEStream(rejectionStream);
          return createSSEResponse(parsedRejectionStream);
        }
      } catch (vectorError) {
        console.error('Vector search error (continuing without context):', vectorError);
        // Continue without context if vector search fails
      }
    }

    // Process with OpenAI
    const stream = await openai.streamChat(enhancedMessages, options);
    const parsedStream = parseSSEStream(stream);

    return createSSEResponse(parsedStream);

  } catch (error) {
    console.error('Chat error:', error);
    return c.json(createErrorResponse(error.message), 400);
  }
});

chat.post('/simple', async (c) => {
  try {
    const { message, systemPrompt, options = {} } = await c.req.json();

    if (!message || typeof message !== 'string') {
      return c.json(createErrorResponse('Message is required'), 400);
    }

    validateOptions(options);

    // Check API key
    if (!c.env.OPENAI_API_KEY) {
      return c.json(createErrorResponse('OpenAI API key not configured'), 500);
    }

    // Initialize services
    const openai = new OpenAIService(c.env);
    const vectorize = new VectorizeService(c.env.CONTENT_VECTORIZE, openai);

    // Search for relevant content
    let enhancedSystemPrompt = systemPrompt || '';

    if (c.env.CONTENT_VECTORIZE) {
      try {
        const searchOptions = options.contentId ? { contentId: options.contentId } : {};
        const contextResult = await vectorize.getContentContext(sanitizeInput(message), 3, searchOptions);

        if (contextResult.hasContext) {
          const contextPrompt = `ê´€ë ¨ ê°•ì˜ ìë£Œ:
${contextResult.context}

ìœ„ ê°•ì˜ ìë£Œë¥¼ ì°¸ê³ í•˜ì—¬ í•™ìŠµìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ê°•ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ì´ ìˆëŠ” ê²½ìš° ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.

IMPORTANT: ë‹µë³€ ì‹œì‘ ë¶€ë¶„ì— ë°˜ë“œì‹œ "ğŸ“š ê°•ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ë“œë¦½ë‹ˆë‹¤.\n\n"ë¥¼ í¬í•¨í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì´ê²ƒì´ ê°•ì˜ ê¸°ë°˜ ë‹µë³€ì„ì„ ì•Œë ¤ì£¼ì„¸ìš”.`;

          enhancedSystemPrompt = enhancedSystemPrompt
            ? enhancedSystemPrompt + '\n\n' + contextPrompt
            : contextPrompt;

          console.log(`âœ… Found ${contextResult.relevantChunks} relevant content chunks for simple chat`);
        } else {
          // No relevant content found - return rejection message directly without AI processing
          console.log('â„¹ï¸ No relevant content found in simple chat - returning rejection message directly');

          const rejectionMessage = 'ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ë“±ë¡ëœ ê°•ì˜ ìë£Œì—ì„œëŠ” í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê°•ì˜ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.';

          // Create a simple SSE stream with the rejection message
          const rejectionStream = new ReadableStream({
            start(controller) {
              const sseData = `data: ${JSON.stringify({
                choices: [{
                  delta: { content: rejectionMessage }
                }]
              })}\n\n`;
              controller.enqueue(new TextEncoder().encode(sseData));
              controller.enqueue(new TextEncoder().encode('data: [DONE]\n\n'));
              controller.close();
            }
          });

          const parsedRejectionStream = parseSSEStream(rejectionStream);
          return createSSEResponse(parsedRejectionStream);
        }
      } catch (vectorError) {
        console.error('Vector search error in simple chat (continuing without context):', vectorError);
        // Continue without context if vector search fails
      }
    }

    // Build messages array with enhanced system prompt
    const messages = [];
    if (enhancedSystemPrompt) {
      messages.push({ role: 'system', content: enhancedSystemPrompt });
    }
    messages.push({ role: 'user', content: sanitizeInput(message) });

    // Process with OpenAI
    const stream = await openai.streamChat(messages, options);
    const parsedStream = parseSSEStream(stream);

    return createSSEResponse(parsedStream);

  } catch (error) {
    console.error('Simple chat error:', error);
    return c.json(createErrorResponse(error.message), 400);
  }
});

export default chat;